{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We’ll demonstrate sequence-to-sequence modeling on a machine translation task.\n",
        "Machine translation is precisely what Transformer was developed for! We’ll start with a\n",
        "recurrent sequence model, and we’ll follow up with the full Transformer architecture."
      ],
      "metadata": {
        "id": "1cgSmxz8sCfh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCP_CmugrdEO",
        "outputId": "36ede5dc-81d5-441b-d3c9-61adcdee7754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-01 05:08:48--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.125.207, 142.250.136.207, 142.250.148.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.125.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-01-01 05:08:48 (195 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        " lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        " english, spanish = line.split(\"\\t\") # Each line contains an English phrase and its Spanish translation, tab-separated.\n",
        " spanish = \"[start] \" + spanish + \" [end]\" # We prepend \"[start]\" and append \"[end]\" to the Spanish sentence, to match the template of decoder\n",
        " text_pairs.append((english, spanish))"
      ],
      "metadata": {
        "id": "sSJlesvgsgMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3THvuQVsyvY",
        "outputId": "d223dec1-6880-45dc-9d43-d3c69678e031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Please don't bother.\", '[start] Por favor, no se moleste. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s shuffle them and split them into the usual training, validation, and test sets"
      ],
      "metadata": {
        "id": "8H92sWcps797"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "vjeoWIYAs8eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s prepare two separate TextVectorization layers: one for English and one\n",
        "for Spanish. We’re going to need to customize the way strings are preprocessed:\n",
        "\n",
        "\n",
        "1) We need to preserve the \"[start]\" and \"[end]\" tokens that we’ve inserted. By\n",
        "default, the characters [ and ] would be stripped, but we want to keep them\n",
        "around so we can tell apart the word “start” and the start token \"[start]\".\n",
        "\n",
        "\n",
        "2) Punctuation is different from language to language! In the Spanish TextVectorization layer, if we’re going to strip punctuation characters, we need to\n",
        "also strip the character ¿."
      ],
      "metadata": {
        "id": "hMLojbtJtJpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important**: Note that for a non-toy translation model, we would treat punctuation characters as separate tokens rather than stripping them, since we would want to be able to generate correctly punctuated sentences. In our case, for simplicity, we’ll get rid of all punctuation."
      ],
      "metadata": {
        "id": "84j--dvTtWjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "C3nHzKBtuOpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "Prepare a custom string\n",
        "standardization function for the\n",
        "Spanish TextVectorization layer:\n",
        "it preserves [ and ] but strips ¿\n",
        "(as well as all other characters\n",
        "from strings.punctuation).\n",
        "\"\"\"\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")"
      ],
      "metadata": {
        "id": "Xwolt0HntOtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string):\n",
        " lowercase = tf.strings.lower(input_string)\n",
        " return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")"
      ],
      "metadata": {
        "id": "79TrXKxOtqVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To keep things simple, we’ll only look at the top 15,000 words in each language, and we’ll restrict sentences to 20 words.\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "# The English layer\n",
        "source_vectorization = layers.TextVectorization(\n",
        " max_tokens=vocab_size,\n",
        " output_mode=\"int\",\n",
        " output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "# The Spanish layer\n",
        "target_vectorization = layers.TextVectorization(\n",
        " max_tokens=vocab_size,\n",
        " output_mode=\"int\",\n",
        " output_sequence_length=sequence_length + 1, # Generate Spanish sentences that have one extra token, since we’ll need to offset the sentence by one step during training.\n",
        " standardize=custom_standardization,\n",
        ")\n",
        "\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "\n",
        "# Learn the vocabulary of each language.\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "id": "1YCQ7QXHty0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can turn our data into a tf.data pipeline.\n",
        "\n",
        "We want it to return a tuple\n",
        "(inputs, target) where inputs is a dict with two keys, “encoder_inputs” (the English\n",
        "sentence) and “decoder_inputs” (the Spanish sentence), and target is the Spanish\n",
        "sentence offset by one step ahead."
      ],
      "metadata": {
        "id": "QPc1q_fOudpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing datasets for the translation task\n",
        "\n",
        "batch_size = 64\n",
        "def format_dataset(eng, spa):\n",
        " eng = source_vectorization(eng)\n",
        " spa = target_vectorization(spa)\n",
        " return ({\n",
        " \"english\": eng,\n",
        " \"spanish\": spa[:, :-1], # The input Spanish sentence doesn’t include the last token to keep inputs and targets at the same length.\n",
        " }, spa[:, 1:]) # The target Spanish sentence is one step ahead. Both are still the same length (20 words).\n",
        "\n",
        "\n",
        "def make_dataset(pairs):\n",
        " eng_texts, spa_texts = zip(*pairs)\n",
        " eng_texts = list(eng_texts)\n",
        " spa_texts = list(spa_texts)\n",
        " dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        " dataset = dataset.batch(batch_size)\n",
        " dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        " return dataset.shuffle(2048).prefetch(16).cache() # Use in-memory caching to speed up preprocessing\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "G_NSZB8uueQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DE3M_gp8vS4U",
        "outputId": "406f4b00-813a-403d-d8e7-3d3440576f9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (64, 20)\n",
            "inputs['spanish'].shape: (64, 20)\n",
            "targets.shape: (64, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is now ready—time to build some models. We’ll start with a recurrent\n",
        "sequence-to-sequence model before moving on to a Transformer."
      ],
      "metadata": {
        "id": "U8UbH0EOvhpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence-to-sequence learning with RNNs"
      ],
      "metadata": {
        "id": "WzpKzbnkPCBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest, naive way to use RNNs to turn a sequence into another sequence is\n",
        "to keep the output of the RNN at each time step."
      ],
      "metadata": {
        "id": "OUJo2dZCPDIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In a proper sequence-to-sequence setup (see figure 11.13), you would first use an\n",
        "RNN (the encoder) to turn the entire source sequence into a single vector (or set of\n",
        "vectors). This could be the last output of the RNN, or alternatively, its final internal\n",
        "state vectors. Then you would use this vector (or vectors) as the initial state of another RNN (the decoder), which would look at elements 0…N in the target sequence, and\n",
        "try to predict step N+1 in the target sequence."
      ],
      "metadata": {
        "id": "TvsLZOurPGtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let’s implement this in Keras with GRU-based encoders and decoders. The choice\n",
        "of GRU rather than LSTM makes things a bit simpler, since GRU only has a single\n",
        "state vector, whereas LSTM has multiple. Let’s start with the encoder."
      ],
      "metadata": {
        "id": "S8c_ibL8PRBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\") # The English source sentence goes here. Specifying the name of the input enables us to fit() the model with a dict of inputs.\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source) # Don’t forget masking: it’s critical in this setup.\n",
        "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x) # Our encoded source sentence is the last output of a bidirectional GRU."
      ],
      "metadata": {
        "id": "Ynu9RzcGviEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'sum': The outputs of the forward and backward passes are summed element-wise. This means that for each time step, the output is the sum of the corresponding forward and backward outputs."
      ],
      "metadata": {
        "id": "4KKYGJKeP7cD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[What is Merge Mode and it's usage](https://chat.openai.com/share/d31fbc96-e223-4263-a887-d5972be62337)"
      ],
      "metadata": {
        "id": "s9Mx30wjPwJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let’s add the decoder—a simple GRU layer that takes as its initial state the\n",
        "encoded source sentence. On top of it, we add a Dense layer that produces for each\n",
        "output step a probability distribution over the Spanish vocabulary."
      ],
      "metadata": {
        "id": "9TIoF0lDQO_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\") # The Spanish target sentence goes here.\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source) # The encoded source sentence serves as the initial state of the decoder GRU.\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) # Predicts the next token\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step) # End-to-end model: maps the source sentence and the target sentence to the target sentence one step in the future"
      ],
      "metadata": {
        "id": "1unX4ZznP2uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "During training, the decoder takes as input the entire target sequence, but thanks to\n",
        "the step-by-step nature of RNNs, it only looks at tokens 0…N in the input to predict token N in the output (which corresponds to the next token in the sequence, since\n",
        "the output is intended to be offset by one step). This means we only use information\n",
        "from the past to predict the future, as we should; otherwise we’d be cheating, and our\n",
        "model would not work at inference time."
      ],
      "metadata": {
        "id": "1_3Rv-PUQ37X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The term \"offset by one step\" means that the target sequence used during training is shifted by one position compared to the input sequence. In other words, the model is trained to predict the next token in the target sequence given the information from the input sequence up to the current step."
      ],
      "metadata": {
        "id": "ksYLwCLOSFM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of sequence-to-sequence models and recurrent neural networks (RNNs), the term \"offset\" refers to the misalignment or time shift between the input and output sequences during training. The idea is that when training a model to generate a sequence of tokens (e.g., translating a sentence from one language to another), you want the model to predict the next token in the output sequence based on the tokens observed so far in the input sequence."
      ],
      "metadata": {
        "id": "BXGbMoIuSIf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        " optimizer=\"rmsprop\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bOpAEQYQ6Zi",
        "outputId": "de1210f0-0822-4c0a-dad9-74acf0214327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "1302/1302 [==============================] - 129s 88ms/step - loss: 4.6863 - accuracy: 0.3182 - val_loss: 3.9132 - val_accuracy: 0.3847\n",
            "Epoch 2/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 3.7411 - accuracy: 0.4133 - val_loss: 3.2834 - val_accuracy: 0.4627\n",
            "Epoch 3/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 3.2296 - accuracy: 0.4713 - val_loss: 2.8846 - val_accuracy: 0.5152\n",
            "Epoch 4/15\n",
            "1302/1302 [==============================] - 110s 84ms/step - loss: 2.8741 - accuracy: 0.5110 - val_loss: 2.6353 - val_accuracy: 0.5485\n",
            "Epoch 5/15\n",
            "1302/1302 [==============================] - 110s 85ms/step - loss: 2.5961 - accuracy: 0.5441 - val_loss: 2.4497 - val_accuracy: 0.5751\n",
            "Epoch 6/15\n",
            "1302/1302 [==============================] - 108s 83ms/step - loss: 2.3744 - accuracy: 0.5722 - val_loss: 2.3132 - val_accuracy: 0.5972\n",
            "Epoch 7/15\n",
            "1302/1302 [==============================] - 110s 85ms/step - loss: 2.1889 - accuracy: 0.5968 - val_loss: 2.2173 - val_accuracy: 0.6116\n",
            "Epoch 8/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 2.0296 - accuracy: 0.6182 - val_loss: 2.1436 - val_accuracy: 0.6235\n",
            "Epoch 9/15\n",
            "1302/1302 [==============================] - 112s 86ms/step - loss: 1.8921 - accuracy: 0.6371 - val_loss: 2.0880 - val_accuracy: 0.6335\n",
            "Epoch 10/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 1.7739 - accuracy: 0.6537 - val_loss: 2.0456 - val_accuracy: 0.6405\n",
            "Epoch 11/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 1.6654 - accuracy: 0.6683 - val_loss: 2.0150 - val_accuracy: 0.6441\n",
            "Epoch 12/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 1.5757 - accuracy: 0.6809 - val_loss: 1.9868 - val_accuracy: 0.6495\n",
            "Epoch 13/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 1.4917 - accuracy: 0.6929 - val_loss: 1.9661 - val_accuracy: 0.6542\n",
            "Epoch 14/15\n",
            "1302/1302 [==============================] - 111s 85ms/step - loss: 1.4228 - accuracy: 0.7032 - val_loss: 1.9451 - val_accuracy: 0.6574\n",
            "Epoch 15/15\n",
            "1302/1302 [==============================] - 110s 85ms/step - loss: 1.3557 - accuracy: 0.7135 - val_loss: 1.9394 - val_accuracy: 0.6586\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78185826add0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We picked accuracy as a crude way to monitor validation-set performance during\n",
        "training. We get to 64% accuracy: on average, the model predicts the next word in the\n",
        "Spanish sentence correctly 64% of the time. However, in practice, next-token accuracy\n",
        "isn’t a great metric for machine translation models, in particular because it makes the\n",
        "assumption that the correct target tokens from 0 to N are already known when predicting token N+1."
      ],
      "metadata": {
        "id": "Za0OEyv7UWdM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reality, during inference, you’re generating the target sentence\n",
        "from scratch, and you can’t rely on previously generated tokens being 100% correct.\n",
        "If you work on a real-world machine translation system, you will likely use “BLEU\n",
        "scores” to evaluate your models—a metric that looks at entire generated sequences\n",
        "and that seems to correlate well with human perception of translation quality."
      ],
      "metadata": {
        "id": "hW1uMNi7UXM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[What is BLEU and how to use it ](https://chat.openai.com/share/38f198b2-21b0-41fe-b1de-6a31f31a0602)"
      ],
      "metadata": {
        "id": "69-zZ-cYVasP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " At last, let’s use our model for inference. We’ll pick a few sentences in the test set\n",
        "and check how our model translates them. We’ll start from the seed token, \"[start]\",\n",
        "and feed it into the decoder model, together with the encoded English source sentence. We’ll retrieve a next-token prediction, and we’ll re-inject it into the decoder\n",
        "repeatedly, sampling one new target token at each iteration, until we get to \"[end]\"\n",
        "or reach the maximum sentence length."
      ],
      "metadata": {
        "id": "4RIGbvuMUY1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "\n",
        "# Prepare a dict to convert token index predictions to string tokens\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20"
      ],
      "metadata": {
        "id": "YOESYW8bUhm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_sequence(input_sentence):\n",
        "  tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "  decoded_sentence = \"[start]\" # Seed token\n",
        "  for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "    next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(next_token_predictions[0, i, :]) # Sample the next token.\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token # Convert the next token prediction to a string and append it to the generated sentence.\n",
        "    if sampled_token == \"[end]\": # Exit condition: either hit max length or sample a stop character\n",
        "      break\n",
        "  return decoded_sentence"
      ],
      "metadata": {
        "id": "2mgQEKPyV6JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        " input_sentence = random.choice(test_eng_texts)\n",
        " print(\"-\")\n",
        " print(input_sentence)\n",
        " print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvvWbNoqWg7X",
        "outputId": "72f21cc7-92dc-4d4b-83d6-b3298c088556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I'm afraid I didn't explain it too well.\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[start] me temo que no tenía tanto así [end]\n",
            "-\n",
            "Cheer up! Things are not as bad as you think.\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "[start] no te [UNK] tan bien como la gente [end]\n",
            "-\n",
            "My watch stopped, so I didn't know the time.\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "[start] mi reloj no se lo di cuenta pero no pudo encontrar [end]\n",
            "-\n",
            "I'm not saying that it's not possible.\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] no estoy diciendo que eso no es posible [end]\n",
            "-\n",
            "I judged you too quickly.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] te [UNK] muy rápido [end]\n",
            "-\n",
            "Do you think I need to go?\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] piensas que me tengo que ir [end]\n",
            "-\n",
            "Tom spent a few years in Boston when he was in college.\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[start] tom se comió tres años en la semana que estaba en la cárcel [end]\n",
            "-\n",
            "Tom was hungry.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] tom estaba hambriento [end]\n",
            "-\n",
            "Can you help me wash these dishes?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] me puedes ayudar a estos animales [end]\n",
            "-\n",
            "I get goose bumps when I see a horror movie.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[start] me [UNK] la [UNK] cuando te [UNK] una película [end]\n",
            "-\n",
            "He is one of my neighbours.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] Él es uno de mis [UNK] [end]\n",
            "-\n",
            "I gave him a present in return for his favor.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] le di un favor para terminar por su ayuda [end]\n",
            "-\n",
            "My sisters were cooking when I came home.\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] mis niños estaban [UNK] cuando llegué a casa [end]\n",
            "-\n",
            "Tom was robbed.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[start] tom era canadiense [end]\n",
            "-\n",
            "Tom felt like dancing.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] tom se sentía como a bailar [end]\n",
            "-\n",
            "Are you through with this book?\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] está usted este libro con este libro [end]\n",
            "-\n",
            "What would you like for breakfast?\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "[start] qué te gustaría [end]\n",
            "-\n",
            "I'd be careful.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] estaría [UNK] [end]\n",
            "-\n",
            "I have a sense of humor.\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "[start] tengo una [UNK] de [UNK] [end]\n",
            "-\n",
            "The injured were transported by ambulance.\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "[start] los [UNK] [UNK] por la [UNK] [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this inference setup, while very simple, is rather inefficient, since we reprocess the entire source sentence and the entire generated target sentence every time\n",
        "we sample a new word. In a practical application, you’d factor the encoder and the\n",
        "decoder as two separate models, and your decoder would only run a single step at\n",
        "each token-sampling iteration, reusing its previous internal state."
      ],
      "metadata": {
        "id": "PVg4hP4tWvc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence-to-sequence learning with Transformer"
      ],
      "metadata": {
        "id": "5XeW51wXa79p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim # Size of the input token vectors\n",
        "    self.dense_dim = dense_dim # Size of the inner dense layer\n",
        "    self.num_heads = num_heads # Number of attention heads\n",
        "    self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),]\n",
        "        )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "  def call(self, inputs, mask=None): # Computation goes in call().\n",
        "    if mask is not None: # The mask that will be generated by the Embedding layer will be 2D, but the attention layer expects to be 3D or 4D, so we expand its rank.\n",
        "      mask = mask[:, tf.newaxis, :]\n",
        "    attention_output = self.attention(\n",
        "    inputs, inputs, attention_mask=mask)\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)\n",
        "  def get_config(self): # Implement serialization so we can save the model.\n",
        "    config = super().get_config()\n",
        "    config.update({\"embed_dim\": self.embed_dim,\"num_heads\": self.num_heads,\"dense_dim\": self.dense_dim,})\n",
        "    return config"
      ],
      "metadata": {
        "id": "ZwyfbsU5Wv8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The TransformerDecoder\n",
        "class TransformerDecoder(layers.Layer):\n",
        " def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.dense_dim = dense_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.dense_proj = keras.Sequential(\n",
        "        [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "        layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm_1 = layers.LayerNormalization()\n",
        "    self.layernorm_2 = layers.LayerNormalization()\n",
        "    self.layernorm_3 = layers.LayerNormalization()\n",
        "    self.supports_masking = True # This attribute ensures that the layer will propagate its input mask to its outputs; masking in Keras is explicitly opt-in. If you pass a mask to a layer that doesn’t\n",
        "    # Implement compute_mask() and that doesn’t expose this supports_masking attribute, that’s an error.\n",
        " def get_config(self):\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"embed_dim\": self.embed_dim,\n",
        "      \"num_heads\": self.num_heads,\n",
        "      \"dense_dim\": self.dense_dim,\n",
        "    })\n",
        "    return config\n",
        " def get_causal_attention_mask(self, inputs):\n",
        "    input_shape = tf.shape(inputs)\n",
        "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "    j = tf.range(sequence_length)\n",
        "    mask = tf.cast(i >= j, dtype=\"int32\") # Generate matrix of shape (sequence_length, sequence_length) with 1s in one half and 0s in the other\n",
        "\n",
        "    # Replicate it along the batch axis to get a matrix of shape (batch_size, sequence_length, sequence_length)\n",
        "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "    mult = tf.concat([tf.expand_dims(batch_size, -1),tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        " def call(self, inputs, encoder_outputs, mask=None):\n",
        "    causal_mask = self.get_causal_attention_mask(inputs)\n",
        "    if mask is not None:\n",
        "      # Prepare the input mask (that describes padding locations in the target sequence).\n",
        "      padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "      padding_mask = tf.minimum(padding_mask, causal_mask) # Merge the two masks together.\n",
        "    attention_output_1 = self.attention_1(query=inputs,value=inputs,key=inputs,attention_mask=causal_mask) # Pass the causal mask to the first attention layer, which performs self-attention over the target sequence.\n",
        "    attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "    attention_output_2 = self.attention_2(\n",
        "    query=attention_output_1,value=encoder_outputs,key=encoder_outputs,attention_mask=padding_mask,) # Pass the combined mask to the second attention layer, which relates the source sequence to the target sequence.\n",
        "    attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "    proj_output = self.dense_proj(attention_output_2)\n",
        "    return self.layernorm_3(attention_output_2 + proj_output)\n"
      ],
      "metadata": {
        "id": "8U5weFlwgPEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Causal padding is absolutely critical to successfully training\n",
        "a sequence-to-sequence Transformer. Unlike an RNN, which looks at its input one\n",
        "step at a time, and thus will only have access to steps 0...N to generate output step N\n",
        "(which is token N+1 in the target sequence), the TransformerDecoder is order-agnostic: it looks at the entire target sequence at once. If it were allowed to use its entire\n",
        "input, it would simply learn to copy input step N+1 to location N in the output. The\n",
        "model would thus achieve perfect training accuracy, but of course, when running\n",
        "inference, it would be completely useless, since input steps beyond N aren’t available."
      ],
      "metadata": {
        "id": "P9fOudPhjyHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fix is simple: we’ll mask the upper half of the pairwise attention matrix to prevent the model from paying any attention to information from the future—only information from tokens 0...N in the target sequence should be used when generating\n",
        "target token N+1. To do this, we’ll add a get_causal_attention_mask(self, inputs)\n",
        "method to our TransformerDecoder to retrieve an attention mask that we can pass to\n",
        "our MultiHeadAttention layers."
      ],
      "metadata": {
        "id": "QGIjIuwYjy5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        " def __init__(self, sequence_length, input_dim, output_dim, **kwargs): # A downside of position embeddings is that the sequence length needs to be known in advance\n",
        "    super().__init__(**kwargs)\n",
        "    self.token_embeddings = layers.Embedding(input_dim=input_dim, output_dim=output_dim) # Prepare an Embedding layer for the token indices.\n",
        "    self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=output_dim) # And another one for the token positions\n",
        "    self.sequence_length = sequence_length\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "\n",
        " def call(self, inputs):\n",
        "    length = tf.shape(inputs)[-1]\n",
        "    positions = tf.range(start=0, limit=length, delta=1) # delta=1: The step size between consecutive values in the sequence is 1.\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = self.position_embeddings(positions)\n",
        "    return embedded_tokens + embedded_positions # Add both embedding vectors together.\n",
        "\n",
        " def compute_mask(self, inputs, mask=None):\n",
        "    # Like the Embedding layer, this layer should be able to generate a mask so we can ignore padding 0s in the inputs. The compute_mask method will called automatically by the framework, and the mask will get propagated\n",
        "    # to the next layer.\n",
        "    return tf.math.not_equal(inputs, 0)\n",
        "\n",
        " def get_config(self):\n",
        "    # Implement serialization so we can save the model.\n",
        "    config = super().get_config()\n",
        "    config.update({\n",
        "      \"output_dim\": self.output_dim,\n",
        "      \"sequence_length\": self.sequence_length,\n",
        "      \"input_dim\": self.input_dim,\n",
        "    })\n",
        "    return config"
      ],
      "metadata": {
        "id": "4HI6_gKfj0yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The end-to-end Transformer is the model we’ll be training. It maps the source\n",
        "sequence and the target sequence to the target sequence one step in the future. It\n",
        "straightforwardly combines the pieces we’ve built so far: PositionalEmbedding layers,\n",
        "the TransformerEncoder, and the TransformerDecoder. Note that both the TransformerEncoder and the TransformerDecoder are shape-invariant, so you could be\n",
        "stacking many of them to create a more powerful encoder or decoder."
      ],
      "metadata": {
        "id": "SYzFqIi7lzgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x) # Encode the source sentence.\n",
        "\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs) # Encode the target sentence and combine it with the encoded source sentence\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)  # Predict a word for each output position\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "metadata": {
        "id": "HeOMzymVl0Pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        " optimizer=\"rmsprop\",\n",
        " loss=\"sparse_categorical_crossentropy\",\n",
        " metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcQLibRwnDNm",
        "outputId": "396afe69-31e3-41f7-bdbd-de97404e926e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "1302/1302 [==============================] - 104s 72ms/step - loss: 3.8006 - accuracy: 0.4393 - val_loss: 2.8752 - val_accuracy: 0.5390\n",
            "Epoch 2/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.8545 - accuracy: 0.5492 - val_loss: 2.5056 - val_accuracy: 0.5930\n",
            "Epoch 3/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.5612 - accuracy: 0.5930 - val_loss: 2.3960 - val_accuracy: 0.6128\n",
            "Epoch 4/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.3961 - accuracy: 0.6199 - val_loss: 2.3333 - val_accuracy: 0.6232\n",
            "Epoch 5/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.2940 - accuracy: 0.6367 - val_loss: 2.2941 - val_accuracy: 0.6312\n",
            "Epoch 6/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.2210 - accuracy: 0.6510 - val_loss: 2.3009 - val_accuracy: 0.6381\n",
            "Epoch 7/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.1633 - accuracy: 0.6624 - val_loss: 2.2831 - val_accuracy: 0.6432\n",
            "Epoch 8/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.1010 - accuracy: 0.6740 - val_loss: 2.2508 - val_accuracy: 0.6520\n",
            "Epoch 9/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.0486 - accuracy: 0.6843 - val_loss: 2.2565 - val_accuracy: 0.6587\n",
            "Epoch 10/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 2.0026 - accuracy: 0.6930 - val_loss: 2.2373 - val_accuracy: 0.6598\n",
            "Epoch 11/30\n",
            "1302/1302 [==============================] - 89s 69ms/step - loss: 1.9659 - accuracy: 0.6996 - val_loss: 2.2498 - val_accuracy: 0.6632\n",
            "Epoch 12/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9340 - accuracy: 0.7058 - val_loss: 2.2421 - val_accuracy: 0.6641\n",
            "Epoch 13/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.9055 - accuracy: 0.7108 - val_loss: 2.2573 - val_accuracy: 0.6669\n",
            "Epoch 14/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8787 - accuracy: 0.7163 - val_loss: 2.2639 - val_accuracy: 0.6693\n",
            "Epoch 15/30\n",
            "1302/1302 [==============================] - 90s 69ms/step - loss: 1.8557 - accuracy: 0.7199 - val_loss: 2.2964 - val_accuracy: 0.6669\n",
            "Epoch 16/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8368 - accuracy: 0.7236 - val_loss: 2.2898 - val_accuracy: 0.6660\n",
            "Epoch 17/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8206 - accuracy: 0.7270 - val_loss: 2.2990 - val_accuracy: 0.6667\n",
            "Epoch 18/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.8038 - accuracy: 0.7299 - val_loss: 2.3263 - val_accuracy: 0.6623\n",
            "Epoch 19/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7847 - accuracy: 0.7334 - val_loss: 2.3196 - val_accuracy: 0.6656\n",
            "Epoch 20/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7673 - accuracy: 0.7366 - val_loss: 2.3337 - val_accuracy: 0.6694\n",
            "Epoch 21/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7520 - accuracy: 0.7393 - val_loss: 2.3346 - val_accuracy: 0.6692\n",
            "Epoch 22/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7389 - accuracy: 0.7414 - val_loss: 2.3247 - val_accuracy: 0.6703\n",
            "Epoch 23/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7208 - accuracy: 0.7448 - val_loss: 2.3942 - val_accuracy: 0.6676\n",
            "Epoch 24/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.7047 - accuracy: 0.7478 - val_loss: 2.3709 - val_accuracy: 0.6688\n",
            "Epoch 25/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6908 - accuracy: 0.7501 - val_loss: 2.3875 - val_accuracy: 0.6692\n",
            "Epoch 26/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6778 - accuracy: 0.7527 - val_loss: 2.3993 - val_accuracy: 0.6715\n",
            "Epoch 27/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6660 - accuracy: 0.7545 - val_loss: 2.4145 - val_accuracy: 0.6678\n",
            "Epoch 28/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6549 - accuracy: 0.7565 - val_loss: 2.4131 - val_accuracy: 0.6720\n",
            "Epoch 29/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6422 - accuracy: 0.7586 - val_loss: 2.4170 - val_accuracy: 0.6716\n",
            "Epoch 30/30\n",
            "1302/1302 [==============================] - 89s 68ms/step - loss: 1.6287 - accuracy: 0.7609 - val_loss: 2.4519 - val_accuracy: 0.6708\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ca67c11ab60>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let’s try using our model to translate never-seen-before English sentences from\n",
        "the test set. The setup is identical to what we used for the sequence-to-sequence RNN\n",
        "model."
      ],
      "metadata": {
        "id": "R55TmfMjnIa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        " tokenized_input_sentence = source_vectorization([input_sentence])\n",
        " decoded_sentence = \"[start]\"\n",
        " for i in range(max_decoded_sentence_length):\n",
        "    tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "    # Sample the next token.\n",
        "    predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "    sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "\n",
        "    # Convert the next token prediction to a string, and append it to the generated sentence.\n",
        "    sampled_token = spa_index_lookup[sampled_token_index]\n",
        "    decoded_sentence += \" \" + sampled_token\n",
        "    if sampled_token == \"[end]\":\n",
        "      break\n",
        " return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        " input_sentence = random.choice(test_eng_texts)\n",
        " print(\"-\")\n",
        " print(input_sentence)\n",
        " print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "BqkTSfgFnJBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b61c02-5b4f-4135-867f-9515884bff39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "I was framed.\n",
            "[start] fui un [UNK] [end]\n",
            "-\n",
            "Tom wanted you to think he'd died.\n",
            "[start] tom quería que [UNK] [end]\n",
            "-\n",
            "We don't believe that Tom will be able to master French.\n",
            "[start] no queremos que tom sea capaz de aprender francés tan pronto [end]\n",
            "-\n",
            "Cancer can be cured if discovered in time.\n",
            "[start] el cáncer puede ser [UNK] en este momento [end]\n",
            "-\n",
            "Please make five copies of this document.\n",
            "[start] por favor hagas cinco [UNK] de este avión [end]\n",
            "-\n",
            "She looked around the room.\n",
            "[start] ella miró alrededor de la habitación [end]\n",
            "-\n",
            "I don't want visitors.\n",
            "[start] no quiero quince [end]\n",
            "-\n",
            "Black and white photos have a special charm.\n",
            "[start] negro y la fotos tienen un viejo [UNK] [end]\n",
            "-\n",
            "The wind blew hard.\n",
            "[start] el viento le pasó duro [end]\n",
            "-\n",
            "We have a colleague in Spain.\n",
            "[start] tenemos una naranja en alemania [end]\n",
            "-\n",
            "You ask questions about everything.\n",
            "[start] tú hagas preguntas acerca de todo [end]\n",
            "-\n",
            "I'm from Canada.\n",
            "[start] soy de canadá [end]\n",
            "-\n",
            "I assure you everything will be ready on time.\n",
            "[start] te [UNK] todo el mundo va a estar listo [end]\n",
            "-\n",
            "I knew Tom was married.\n",
            "[start] sabía que tom estaba casada [end]\n",
            "-\n",
            "When he retired, his son took over the business.\n",
            "[start] cuando él se paró a su hijo se tomó el negocio [end]\n",
            "-\n",
            "It's not so far.\n",
            "[start] no está tan lejos [end]\n",
            "-\n",
            "The letter is inside the envelope.\n",
            "[start] la carta está en sobre [end]\n",
            "-\n",
            "You betrayed us.\n",
            "[start] nos traicionó [end]\n",
            "-\n",
            "What's this key for?\n",
            "[start] esto es [UNK] por favor [end]\n",
            "-\n",
            "Tom and Mary are the same age.\n",
            "[start] tom y mary tienen la misma edad [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subjectively, the Transformer seems to perform significantly better than the GRUbased translation model. It’s still a toy model, but it’s a better toy model."
      ],
      "metadata": {
        "id": "F_8w799foSbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the source sentence wasn’t\n",
        "gendered, this translation assumes\n",
        "a male speaker. Keep in mind that\n",
        "translation models will often make\n",
        "unwarranted assumptions about\n",
        "their input data, which leads to\n",
        "algorithmic bias. In the worst\n",
        "cases, a model might hallucinate\n",
        "memorized information that has\n",
        "nothing to do with the data it’s\n",
        "currently processing"
      ],
      "metadata": {
        "id": "2-IMWwBLoO77"
      }
    }
  ]
}